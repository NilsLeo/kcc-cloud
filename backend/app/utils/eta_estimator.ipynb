{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "NONE"
      },
      "source": [
        "\"\"\"\n",
        "ETA Estimation Algorithm for Manga/Comic Conversion\n",
        "\n",
        "Based on empirical data:\n",
        "- Witch Hat Atelier v03: 189 pages, ~422 seconds total\n",
        "  - MuPDF extraction: 141 seconds (33.4%)\n",
        "  - Image processing: 281 seconds (66.6%)\n",
        "  - HTML building: 0.1 seconds (negligible)\n",
        "\n",
        "Performance factors:\n",
        "- File type (PDF vs archives vs images)\n",
        "- File size (correlates with page count and image quality)\n",
        "- Device profile resolution (higher = more processing)\n",
        "- Advanced options (quality, upscaling, color, etc.)\n",
        "- Available system resources\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import sqlalchemy\n",
        "from sqlalchemy import func, text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "import json\n",
        "from typing import Dict, Any, Optional\n",
        "from database import SessionLocal, ConversionJob\n",
        "from utils.enhanced_logger import setup_enhanced_logging, log_with_context\n",
        "\n",
        "logger = setup_enhanced_logging()\n",
        "\n",
        "\n",
        "CATEGORICAL_FEATURES = ['device_profile']\n",
        "NUMERICAL_FEATURES = ['input_file_size', 'output_file_size']\n",
        "TARGET_FEATURE = ['actual_duration']\n",
        "\n",
        "def retrieve_data():\n",
        "    from database.models import get_db_session\n",
        "    session = get_db_session()\n",
        "    try:\n",
        "        # Execute raw SQL and fetch results\n",
        "        result = session.execute(sqlalchemy.text(\"SELECT * FROM conversion_jobs\"))\n",
        "        columns = result.keys()\n",
        "        rows = result.fetchall()\n",
        "        # Convert to DataFrame manually\n",
        "        df = pd.DataFrame(rows, columns=columns)\n",
        "        return df\n",
        "    finally:\n",
        "        session.close()\n",
        "\n",
        "def sanitize_data(df):\n",
        "    df = df[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + TARGET_FEATURE]\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "    return df\n",
        "\n",
        "def train_model():\n",
        "    df = sanitize_data(retrieve_data())\n",
        "    X = df[NUMERICAL_FEATURES + CATEGORICAL_FEATURES]\n",
        "    y = df[TARGET_FEATURE]\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), CATEGORICAL_FEATURES)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "    models = {\n",
        "        \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "        \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "        \"Linear Regression\": LinearRegression()\n",
        "    }\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    best_model_name = None\n",
        "    best_mse = float('inf')\n",
        "    best_pipeline = None\n",
        "\n",
        "    for name, model in models.items():\n",
        "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('regressor', model)])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        print(f\"{name} - MSE: {mse:.2f}\")\n",
        "        if mse < best_mse:\n",
        "            best_mse = mse\n",
        "            best_model_name = name\n",
        "            best_pipeline = pipeline\n",
        "\n",
        "    print(f\"Best model: {best_model_name} with MSE {best_mse:.2f}\")\n",
        "\n",
        "    # Save best model\n",
        "    joblib.dump(best_pipeline, \"best_model.pkl\")\n",
        "\n",
        "def should_retrain_model(model_path=\"best_model.pkl\", max_age_days=1):\n",
        "    \"\"\"\n",
        "    Check if the model should be retrained based on its age.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        return True  # Train if model doesn't exist\n",
        "\n",
        "    # Check the age of the model file\n",
        "    model_age_seconds = time.time() - os.path.getmtime(model_path)\n",
        "    model_age_days = model_age_seconds / (60 * 60 * 24)\n",
        "\n",
        "    if model_age_days > max_age_days:\n",
        "        return True  # Retrain if model is too old\n",
        "\n",
        "    return False\n",
        "\n",
        "def estimate_eta(conversion_job: ConversionJob, output_file_size: Optional[int] = None) -> float:\n",
        "    \"\"\"\n",
        "    Estimate the conversion time (ETA) for a given job.\n",
        "    If the model is old or doesn't exist, it retrains it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the model needs retraining\n",
        "        if should_retrain_model():\n",
        "            log_with_context(\n",
        "                logger, 'info', 'ETA model requires training.',\n",
        "                job_id=conversion_job.id,\n",
        "                user_id=conversion_job.license_key\n",
        "            )\n",
        "            train_model()\n",
        "            \n",
        "        loaded_model = joblib.load(\"best_model.pkl\")\n",
        "\n",
        "        # Heuristic for output file size if not provided\n",
        "        if output_file_size is None:\n",
        "            output_file_size = conversion_job.input_file_size * 0.8\n",
        "\n",
        "        single_row = pd.DataFrame({\n",
        "            \"input_file_size\": [conversion_job.input_file_size],\n",
        "            \"output_file_size\": [output_file_size],\n",
        "            \"device_profile\": [conversion_job.device_profile]\n",
        "        })\n",
        "\n",
        "        eta = loaded_model.predict(single_row)[0]\n",
        "\n",
        "        # Convert numpy type to Python float for JSON serialization and proper rounding\n",
        "        eta_float = float(eta)\n",
        "\n",
        "        log_with_context(\n",
        "            logger, 'info', 'Successfully estimated ETA',\n",
        "            job_id=conversion_job.id,\n",
        "            user_id=conversion_job.license_key,\n",
        "            estimated_eta=eta_float\n",
        "        )\n",
        "\n",
        "        return max(0.0, eta_float)\n",
        "\n",
        "    except Exception as e:\n",
        "        log_with_context(\n",
        "            logger, 'error', f'An error occurred during ETA estimation: {e}',\n",
        "            job_id=conversion_job.id,\n",
        "            user_id=conversion_job.license_key,\n",
        "            error_type=type(e).__name__\n",
        "        )\n",
        "        # Fallback heuristic in case of any error\n",
        "        return (conversion_job.input_file_size / 50000)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}