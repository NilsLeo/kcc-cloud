{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "NONE"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "This module defines Celery tasks that can be executed asynchronously by worker processes.\n",
        "Tasks are queued via Redis and processed independently from the main Flask application.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import tempfile\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "from celery_config import celery_app\n",
        "from database.models import get_db_session, ConversionJob\n",
        "from utils.enums.job_status import JobStatus\n",
        "from utils.storage import storage\n",
        "from utils.command_generator import generate_kcc_command\n",
        "from utils.socketio_broadcast import broadcast_queue_update\n",
        "from utils.redis_job_store import RedisJobStore\n",
        "from utils.generated_estimator import estimate_from_job\n",
        "\n",
        "# Setup simple logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@celery_app.task(\n",
        "    bind=True,\n",
        "    name=\"mangaconverter.convert_comic\",\n",
        "    max_retries=3,\n",
        "    default_retry_delay=60,\n",
        "    autoretry_for=(Exception,),\n",
        "    retry_backoff=True,\n",
        "    retry_backoff_max=600,\n",
        "    retry_jitter=True,\n",
        ")\n",
        "def convert_comic_task(self, job_id):\n",
        "    \"\"\"\n",
        "    Celery task for converting manga/comic files to e-reader formats.\n",
        "\n",
        "    Args:\n",
        "        self: Celery task instance (bound via bind=True)\n",
        "        job_id (str): Unique job identifier (UUID)\n",
        "\n",
        "    Returns:\n",
        "        dict: Task result with status and job_id\n",
        "    \"\"\"\n",
        "    db = get_db_session()\n",
        "    temp_dir = None\n",
        "\n",
        "    try:\n",
        "        job = db.query(ConversionJob).filter_by(id=job_id).first()\n",
        "\n",
        "        if not job:\n",
        "            raise ValueError(f\"Job {job_id} not found in database\")\n",
        "\n",
        "        logger.info(f\"Starting conversion for job {job_id}: {job.input_filename}\")\n",
        "\n",
        "        job.status = JobStatus.PROCESSING\n",
        "        job.processing_at = datetime.now(timezone.utc)\n",
        "        job.processing_started_at = datetime.now(timezone.utc)\n",
        "        db.commit()\n",
        "\n",
        "        # Mirror basic PROCESSING state to Redis (broadcast deferred until ETA is known)\n",
        "        try:\n",
        "            RedisJobStore.update_job(\n",
        "                job_id,\n",
        "                {\n",
        "                    \"status\": JobStatus.PROCESSING.value,\n",
        "                    \"processing_at\": job.processing_at,\n",
        "                    \"processing_started_at\": job.processing_started_at,\n",
        "                },\n",
        "            )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Get uploaded file path\n",
        "        input_path = storage.get_upload_path(job_id)\n",
        "        if not input_path or not os.path.exists(input_path):\n",
        "            raise FileNotFoundError(f\"Input file not found for job {job_id}\")\n",
        "\n",
        "        # Precompute page count for all supported formats\n",
        "        def _compute_page_count(path: str) -> int:\n",
        "            try:\n",
        "                p = Path(path)\n",
        "                suffix = p.suffix.lower()\n",
        "                image_exts = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n",
        "\n",
        "                # PDF\n",
        "                if suffix == \".pdf\":\n",
        "                    try:\n",
        "                        try:\n",
        "                            import fitz  # PyMuPDF\n",
        "\n",
        "                            with fitz.open(path) as doc:\n",
        "                                return int(doc.page_count)\n",
        "                        except Exception:\n",
        "                            import pymupdf  # type: ignore\n",
        "\n",
        "                            with pymupdf.open(path) as doc:  # pragma: no cover\n",
        "                                return int(doc.page_count)\n",
        "                    except Exception:\n",
        "                        return 0\n",
        "\n",
        "                # EPUB\n",
        "                if suffix == \".epub\":\n",
        "                    import zipfile\n",
        "\n",
        "                    try:\n",
        "                        with zipfile.ZipFile(path, \"r\") as zf:\n",
        "                            return sum(\n",
        "                                1 for n in zf.namelist() if Path(n).suffix.lower() in image_exts\n",
        "                            )\n",
        "                    except Exception:\n",
        "                        return 0\n",
        "\n",
        "                # ZIP, CBZ\n",
        "                if suffix in {\".zip\", \".cbz\"}:\n",
        "                    import zipfile\n",
        "\n",
        "                    try:\n",
        "                        with zipfile.ZipFile(path, \"r\") as zf:\n",
        "                            return sum(\n",
        "                                1 for n in zf.namelist() if Path(n).suffix.lower() in image_exts\n",
        "                            )\n",
        "                    except Exception:\n",
        "                        return 0\n",
        "\n",
        "                # RAR, CBR\n",
        "                if suffix in {\".rar\", \".cbr\"}:\n",
        "                    try:\n",
        "                        import rarfile\n",
        "\n",
        "                        with rarfile.RarFile(path, \"r\") as rf:\n",
        "                            return sum(\n",
        "                                1 for n in rf.namelist() if Path(n).suffix.lower() in image_exts\n",
        "                            )\n",
        "                    except Exception:\n",
        "                        return 0\n",
        "\n",
        "                # 7Z, CB7\n",
        "                if suffix in {\".7z\", \".cb7\"}:\n",
        "                    try:\n",
        "                        import py7zr\n",
        "\n",
        "                        with py7zr.SevenZipFile(path, \"r\") as szf:\n",
        "                            return sum(\n",
        "                                1 for n in szf.getnames() if Path(n).suffix.lower() in image_exts\n",
        "                            )\n",
        "                    except Exception:\n",
        "                        return 0\n",
        "\n",
        "                # Directory\n",
        "                if os.path.isdir(path):\n",
        "                    total = 0\n",
        "                    for root, _dirs, files in os.walk(path):\n",
        "                        total += sum(1 for f in files if Path(f).suffix.lower() in image_exts)\n",
        "                    return total\n",
        "            except Exception:\n",
        "                return 0\n",
        "            return 0\n",
        "\n",
        "        # Compute page count and estimate processing time\n",
        "        page_count = _compute_page_count(input_path)\n",
        "        logger.info(f\"Computed page_count={page_count} for job {job_id} from {input_path}\")\n",
        "\n",
        "        job.page_count = page_count\n",
        "\n",
        "        file_size = os.path.getsize(input_path)\n",
        "        job_data = {\n",
        "            \"page_count\": page_count,\n",
        "            \"file_size\": file_size,\n",
        "            \"filename\": job.input_filename or job.original_filename,\n",
        "            \"advanced_options\": job.get_options_dict(),\n",
        "        }\n",
        "        projected_eta = estimate_from_job(job_data)\n",
        "        logger.info(f\"Estimated processing time: {projected_eta}s for job {job_id}\")\n",
        "\n",
        "        # Store in database\n",
        "        job.estimated_duration_seconds = projected_eta\n",
        "        db.commit()\n",
        "\n",
        "        # Store in Redis for frontend - provide absolute ETA timestamp (eta_at)\n",
        "        # and projected seconds for fallback\n",
        "        try:\n",
        "            eta_at = (\n",
        "                job.processing_started_at or job.processing_at or datetime.now(timezone.utc)\n",
        "            ) + timedelta(seconds=int(projected_eta or 0))\n",
        "            RedisJobStore.update_job(\n",
        "                job_id,\n",
        "                {\n",
        "                    \"processing_progress\": {\n",
        "                        \"eta_at\": eta_at.isoformat(),\n",
        "                        \"projected_eta\": projected_eta,\n",
        "                    }\n",
        "                },\n",
        "            )\n",
        "            logger.info(f\"Updated Redis with projected_eta={projected_eta}s for job {job_id}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Now broadcast update so frontend can start ticker (ETA is available)\n",
        "        broadcast_queue_update()\n",
        "\n",
        "        # Create temporary directory for conversion\n",
        "        temp_dir = tempfile.mkdtemp(prefix=f\"kcc_{job_id}_\")\n",
        "        logger.info(f\"Created temp directory: {temp_dir}\")\n",
        "\n",
        "        # Generate KCC command\n",
        "        options = job.get_options_dict()\n",
        "        kcc_command = generate_kcc_command(\n",
        "            input_path=input_path,\n",
        "            output_dir=temp_dir,\n",
        "            device_profile=job.device_profile,\n",
        "            options=options,\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Running KCC command: {' '.join(kcc_command)}\")\n",
        "\n",
        "        # Run KCC conversion\n",
        "        start_time = datetime.now(timezone.utc)\n",
        "        process = subprocess.Popen(\n",
        "            kcc_command,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True,\n",
        "            cwd=temp_dir,\n",
        "        )\n",
        "\n",
        "        # Stream output\n",
        "        for line in process.stdout:\n",
        "            logger.info(f\"KCC: {line.rstrip()}\")\n",
        "\n",
        "        process.wait()\n",
        "\n",
        "        if process.returncode != 0:\n",
        "            raise RuntimeError(f\"KCC conversion failed with return code {process.returncode}\")\n",
        "\n",
        "        # Calculate conversion duration\n",
        "        end_time = datetime.now(timezone.utc)\n",
        "        job.actual_duration = int((end_time - start_time).total_seconds())\n",
        "\n",
        "        # Find output file\n",
        "        output_files = list(Path(temp_dir).glob(\"*\"))\n",
        "        output_files = [f for f in output_files if f.is_file() and not f.name.startswith(\".\")]\n",
        "\n",
        "        if not output_files:\n",
        "            raise FileNotFoundError(\"No output file produced by KCC\")\n",
        "\n",
        "        output_file = output_files[0]\n",
        "        output_filename = output_file.name\n",
        "\n",
        "        logger.info(f\"Conversion complete. Output file: {output_filename}\")\n",
        "\n",
        "        # Save output to storage\n",
        "        storage.save_output(str(output_file), job_id, output_filename)\n",
        "\n",
        "        # Update job in database\n",
        "        job.output_filename = output_filename\n",
        "        job.output_file_size = storage.get_file_size(storage.get_output_path(job_id))\n",
        "        job.status = JobStatus.COMPLETE\n",
        "        job.completed_at = datetime.now(timezone.utc)\n",
        "\n",
        "        # Calculate actual duration (handle naive vs aware datetimes safely)\n",
        "        try:\n",
        "            start_dt = job.processing_started_at or job.processing_at\n",
        "            end_dt = job.completed_at\n",
        "            if start_dt and end_dt:\n",
        "                if (hasattr(start_dt, \"tzinfo\") and start_dt.tzinfo) and (\n",
        "                    hasattr(end_dt, \"tzinfo\") and end_dt.tzinfo\n",
        "                ):\n",
        "                    job.actual_duration = int((end_dt - start_dt).total_seconds())\n",
        "                else:\n",
        "                    # Normalize to naive before subtracting\n",
        "                    job.actual_duration = int(\n",
        "                        (\n",
        "                            (end_dt.replace(tzinfo=None) if hasattr(end_dt, \"replace\") else end_dt)\n",
        "                            - (\n",
        "                                start_dt.replace(tzinfo=None)\n",
        "                                if hasattr(start_dt, \"replace\")\n",
        "                                else start_dt\n",
        "                            )\n",
        "                        ).total_seconds()\n",
        "                    )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        db.commit()\n",
        "\n",
        "        # Mirror to Redis\n",
        "        try:\n",
        "            RedisJobStore.update_job(\n",
        "                job_id,\n",
        "                {\n",
        "                    \"status\": JobStatus.COMPLETE.value,\n",
        "                    \"completed_at\": job.completed_at,\n",
        "                    \"output_filename\": job.output_filename,\n",
        "                    \"output_file_size\": job.output_file_size or 0,\n",
        "                    \"actual_duration\": job.actual_duration or 0,\n",
        "                },\n",
        "            )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Broadcast update\n",
        "        broadcast_queue_update()\n",
        "\n",
        "        logger.info(f\"Job {job_id} completed successfully\")\n",
        "\n",
        "        return {\"status\": \"success\", \"job_id\": job_id, \"output_filename\": output_filename}\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Job {job_id} failed: {str(e)}\", exc_info=True)\n",
        "\n",
        "        # Update job status to ERRORED\n",
        "        try:\n",
        "            job = db.query(ConversionJob).filter_by(id=job_id).first()\n",
        "            if job:\n",
        "                job.status = JobStatus.ERRORED\n",
        "                job.errored_at = datetime.utcnow()\n",
        "                job.error_message = str(e)\n",
        "                db.commit()\n",
        "\n",
        "                # Mirror to Redis\n",
        "                try:\n",
        "                    RedisJobStore.update_job(\n",
        "                        job_id,\n",
        "                        {\n",
        "                            \"status\": JobStatus.ERRORED.value,\n",
        "                            \"errored_at\": job.errored_at,\n",
        "                            \"error_message\": job.error_message,\n",
        "                        },\n",
        "                    )\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                # Broadcast update\n",
        "                broadcast_queue_update()\n",
        "        except Exception as db_error:\n",
        "            logger.error(f\"Failed to update job status: {db_error}\")\n",
        "\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        db.close()\n",
        "\n",
        "        # Clean up temporary directory\n",
        "        if temp_dir and os.path.exists(temp_dir):\n",
        "            try:\n",
        "                shutil.rmtree(temp_dir)\n",
        "                logger.info(f\"Cleaned up temp directory: {temp_dir}\")\n",
        "            except Exception as cleanup_error:\n",
        "                logger.warning(f\"Failed to clean up temp directory: {cleanup_error}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}